{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid19 Tweets Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import unquote\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"talk\")\n",
    "figsize = (15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Set the correct filenames and paths to the input data file (download first)\n",
    "* `TweetsCOV19_file` from https://data.gesis.org/tweetscov19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetsCOV19_file = \"../data/tweetsCov19/TweetsCOV19.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load Tweets Covid-19 data (~45 min. to 1 hour, due to slow timestamp conversion. Comment out converter for MUCH faster read)\n",
    "colnames = [\"TweetID\", \"Username\", \"date\", \"Followers\", \"Friends\", \"Retweets\", \"Favorites\", \"Entities\", \"Sentiment\", \"Mentions\", \"Hashtags\", \"URLs\"]\n",
    "df = pd.read_csv(TweetsCOV19_file, sep='\\t', header=None, names=colnames,\n",
    "                 na_values=[\"null;\"], converters={\"date\": pd.Timestamp},\n",
    "                 nrows=None)\n",
    "df.set_index(\"date\", drop=True, inplace=True)\n",
    "print(df.shape)\n",
    "print(df.index.min(), df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# separate positive from negative sentiment score\n",
    "sentiment = df[\"Sentiment\"].apply(lambda s: s.split())\n",
    "df[\"Sentiment_positive\"] = sentiment.apply(lambda l: int(l[0]))\n",
    "df[\"Sentiment_negative\"] = sentiment.apply(lambda l: int(l[1]))\n",
    "df.drop(columns=[\"Sentiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with pickle so it can be read faster next time\n",
    "%time df.to_pickle(\"../data/tweetsCov19/TweetsCOV19_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickled data\n",
    "%time df = pd.read_pickle(\"../data/tweetsCov19/TweetsCOV19_df.pkl\")\n",
    "print(df.shape)\n",
    "print(df.index.min(), df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize entity lists to empty lists (~15 sec.)\n",
    "%time df[\"Entities_list\"] = np.empty((len(df), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# get a list of all entities (~30 sec.)\n",
    "nonna = ~df[\"Entities\"].isna()\n",
    "df.loc[nonna,\"Entities_list\"] = df[\"Entities\"][nonna].apply(lambda s: [i.split(':')[1] for i in s.split(';')[:-1]])\n",
    "entities = pd.Series(itertools.chain.from_iterable(df[\"Entities_list\"]))\n",
    "entities.name = \"Entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_counts = entities.value_counts()\n",
    "entities_counts_rel = entities_counts / df.shape[0] # dividy by total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_cutoff_upper = 0.009 # chosen as to remove some uninteresting entities\n",
    "freq_cutoff_lower = 0.001\n",
    "frequent_entities = entities_counts_rel[(entities_counts_rel <= freq_cutoff_upper) & (entities_counts_rel >= freq_cutoff_lower)]\n",
    "frequent_entities.drop(\"Spotify\", inplace=True) # due to strange behaviour (single large peak in December 2019)\n",
    "print(f\"There are {entities_counts.shape[0]} distinct entities in the dataset.\")\n",
    "print(f\"Only {frequent_entities.shape[0]} entities appear between {freq_cutoff_lower*100} % and {freq_cutoff_upper*100:.1f} % of all tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The frequency of entities decays sharply, i.e. there are only few tweets that appear frequently\")\n",
    "s = 0  # start index of entity to display\n",
    "n = 50 # number of entities to display\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(entities_counts_rel.values[s:n+s], label=\"Entity Frequency\")\n",
    "plt.plot([0,n-1], 2*[freq_cutoff_upper], \"--\", label=f\"Upper Cutoff ({freq_cutoff_upper})\")\n",
    "plt.plot([0,n-1], 2*[freq_cutoff_lower], \"--\", label=f\"Lower Cutoff ({freq_cutoff_lower})\")\n",
    "plt.xticks(range(n), [unquote(l) for l in entities_counts.index[s:n+s]], rotation=90)\n",
    "plt.ylabel(\"Frequency of Appearance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/entity_frequency.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Usage Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time daily_sum = df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time weekly_sum = df.resample('W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time monthly_sum = df.resample('M').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time daily_mean = df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time weekly_mean = df.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time monthly_mean = df.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sum[frequent_entities.index[28:29]].plot(figsize=figsize, ylabel=\"Tweets per Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "plt.plot(weekly_sum[frequent_entities.index].sum(1))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Tweets per week\")\n",
    "plt.savefig(\"../figures/tweets_per_week.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_on_period(data, period=None, stdperiod=None, std_only_if_peak=True):\n",
    "    \"\"\"\n",
    "    Normalize data on specified period\n",
    "    \n",
    "    period: pandas datetime indexing string for the period that is used for the normalization\n",
    "    stdperiod: pandas datetime intexing string for the preiod that is used for std computation, defaults to period value\n",
    "    std_only_if_peak: if true only use stdperiod if the global peak is in this period, otherwise use period\n",
    "    \"\"\"\n",
    "    \n",
    "    if period is None:\n",
    "        data_mean = data.mean()\n",
    "    else:\n",
    "        data_mean = data[period].mean()\n",
    "        \n",
    "    if stdperiod is None:\n",
    "        data_std = data.std()\n",
    "    else:\n",
    "        if not std_only_if_peak:\n",
    "            data_std = data[stdperiod].std()\n",
    "        else:\n",
    "            # if global max falls within stdperiod\n",
    "            if data[stdperiod].max() == data.max():\n",
    "                data_std = data[stdperiod].std()\n",
    "            else:\n",
    "                if period is None:\n",
    "                    data_std = data.std()\n",
    "                else:\n",
    "                    data_std = data[period].std()\n",
    "                \n",
    "    \n",
    "    data -= data_mean\n",
    "    data /= data_std\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def correlate(series1, series2):\n",
    "    corr = np.correlate(series1, series2, mode=\"same\")\n",
    "    corrmax = corr.max()\n",
    "    corr_offset = (series1.shape[0] // 2) - corr.argmax()\n",
    "    corrtime    = corr_offset * series1.index.freq\n",
    "    return corrmax, corrtime\n",
    "\n",
    "\n",
    "correlations = np.zeros(2*[frequent_entities.shape[0]], dtype=float)\n",
    "corrtimes    = np.zeros_like(correlations, dtype=pd.Timedelta)\n",
    "correlations = pd.DataFrame(correlations, index=frequent_entities.index, columns=frequent_entities.index)\n",
    "corrtimes    = pd.DataFrame(corrtimes,    index=frequent_entities.index, columns=frequent_entities.index)\n",
    "\n",
    "for entity1 in tqdm(frequent_entities.index):\n",
    "    series1 = daily_sum[entity1].copy()\n",
    "    series1 = normalize_on_period(series1, period=\"2019\", stdperiod=\"2020\")\n",
    "    \n",
    "    for entity2 in frequent_entities.index:\n",
    "        series2 = daily_sum[entity2].copy()\n",
    "        series2 = normalize_on_period(series2, period=\"2019\", stdperiod=\"2020\")\n",
    "        \n",
    "        corrmax, corrtime = correlate(series1, series2)\n",
    "        correlations.loc[entity1, entity2] = corrmax\n",
    "        corrtimes.loc[entity1, entity2]    = corrtime.delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which entities appear together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Japan\", \"Taiwan\", \"Russia\",\n",
    "             \"Canada\", \"South_Korea\", \"Iran\", \"Australia\", \"United_States\", \"Turkey\",\n",
    "             \"Ukraine\", \"Nigeria\", \"Lagos\", \"Hong_Kong\", \"Nigeria\", \"Europe\", \"India\",\n",
    "             \"Kashmir\", \"Pakistan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add integer column to count entity usage\n",
    "for entity in tqdm(frequent_entities.index, desc=\"Extracting entities in int cols\"):\n",
    "    df[entity] = df[\"Entities_list\"].apply(lambda l: int(entity in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with pickle so it can be read faster next time\n",
    "%time df.to_pickle(\"../data/tweetsCov19/TweetsCOV19_df_intent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = pd.read_pickle(\"../data/tweetsCov19/TweetsCOV19_df_intent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_entities = df[frequent_entities.index].copy()\n",
    "df_entities = df_entities.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the \"togetherness\" as the frequency that entity1 appears together with entity2 times the opposite frequency (entity2 together with entity1)\n",
    "togetherness = np.zeros(2*[frequent_entities.shape[0]], dtype=float)\n",
    "togetherness = pd.DataFrame(togetherness, index=frequent_entities.index, columns=frequent_entities.index)\n",
    "\n",
    "for entity1 in tqdm(frequent_entities.index):\n",
    "    ent1_bool = df[entity1] == 1\n",
    "    ent1_sum  = df[entity1].sum()\n",
    "    for entity2 in frequent_entities.index:\n",
    "        togetherness.loc[entity1, entity2] = (((ent1_bool) & (df[entity2] == 1)).sum()**2 / (ent1_sum * df[entity2].sum()))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time daily_sum_corr = daily_sum[frequent_entities.index].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8.5 minutes\n",
    "%time tweet_corr = df[frequent_entities.index].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove countries from togetherness, because we don't care about them\n",
    "not_country    = [False if i in countries else True for i in togetherness.index]\n",
    "togetherness   = togetherness.loc[not_country, not_country]\n",
    "not_country    = [False if i in countries else True for i in tweet_corr.index]\n",
    "tweet_corr     = tweet_corr.loc[not_country, not_country]\n",
    "not_country    = [False if i in countries else True for i in daily_sum_corr.index]\n",
    "daily_sum_corr = daily_sum_corr.loc[not_country, not_country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_sum.shape)\n",
    "print(daily_sum_corr.shape)\n",
    "daily_sum_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(tweet_corr.shape)\n",
    "tweet_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "plt.hist(daily_sum_corr.values.flatten(), bins=50, range=(0,1.0), log=True)\n",
    "plt.xlabel(\"Entity correlations based on daily sum timeseries\")\n",
    "plt.ylabel(\"Count (Log Scale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "plt.hist(tweet_corr.values.flatten(), bins=50, range=(-0.01,0.3), log=True)\n",
    "plt.xlabel(\"Entity correlations based on individual tweets\")\n",
    "plt.ylabel(\"Count (Log Scale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "plt.hist(togetherness.values.flatten(), bins=50, range=(0,0.3), log=True)\n",
    "plt.xlabel(\"Togetherness (T) of Entities\")\n",
    "plt.ylabel(\"Count (Log Scale)\")\n",
    "plt.savefig(\"../figures/togetherness_histogram.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corr(corr, entity1, entity2):\n",
    "    print(f\"T({entity1},{entity2}) = {corr.loc[entity1,entity2]:.2f}\")\n",
    "\n",
    "print_corr(togetherness, \"Bill_Gates\", \"5G\")\n",
    "print_corr(togetherness, \"Bill_Gates\", \"Vaccine\")\n",
    "print_corr(togetherness, \"Toilet_paper\", \"Panic_buying\")\n",
    "print_corr(togetherness, \"President_of_the_United_States\", \"Men_who_have_sex_with_men\")\n",
    "print_corr(togetherness, \"President_of_the_United_States\", \"Fake_news\")\n",
    "print_corr(togetherness, \"Donald_Trump\", \"Men_who_have_sex_with_men\")\n",
    "print_corr(togetherness, \"Donald_Trump\", \"Fake_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = 0.9\n",
    "lower_limit = 0.01\n",
    "pairs = list(zip(*np.where((togetherness > lower_limit) & (togetherness < upper_limit))))\n",
    "pairs = set([tuple(sorted(list(p))) for p in pairs])\n",
    "pairs = [(togetherness.index[p[0]], togetherness.index[p[1]]) for p in pairs]\n",
    "\n",
    "pairs = pd.DataFrame(pairs, columns=[\"entity1\", \"entity2\"])\n",
    "pairs[\"togetherness\"] = pairs.apply(lambda row: togetherness.loc[row.entity1, row.entity2], axis=1)\n",
    "\n",
    "# remove pairs that are both cuntries and sort by togetherness\n",
    "pairs = pairs[~pairs.apply(lambda row: (row.entity1 in countries) & (row.entity2 in countries), axis=1)]\n",
    "pairs.sort_values(\"togetherness\", inplace=True, ascending=False)\n",
    "pairs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "with pd.option_context('display.max_rows', pairs.shape[0]):\n",
    "    display(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_sum(entitylists=None, plotname=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for entities in entitylists:\n",
    "        plt.plot(daily_sum[entities].sum(1), label=f\"{entities}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Daily Number of Tweets\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if plotname is not None:\n",
    "        plt.savefig(f\"../figures/{plotname}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities1 = [\"Bill_Gates\"]\n",
    "entities2 = [\"Vaccine\"]\n",
    "entities3 = [\"5G\"]\n",
    "plot_daily_sum((entities1, entities2, entities3), \"time_evolution_bill_gates_vaccine_5g.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities1 = [\"Toilet_paper\"]\n",
    "entities2 = [\"Panic_buying\"]\n",
    "plot_daily_sum((entities1, entities2), \"time_evolution_toilet_paper_panic_buying.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities1 = [\"President_of_the_United_States\", \"Donald_Trump\"]\n",
    "entities2 = [\"Men_who_have_sex_with_men\", \"Fake_news\"]\n",
    "plot_daily_sum((entities1, entities2), \"time_evolution_president_msm_fake_news.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average sentiment by entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_mean_sentiment = weekly_mean[[\"Sentiment_positive\", \"Sentiment_negative\"]].abs()\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(weekly_mean_sentiment[\"Sentiment_positive\"], \"g\", label=\"Positive Sentiment Score\")\n",
    "plt.plot(weekly_mean_sentiment[\"Sentiment_negative\"], \"r\", label=\"Negative Sentiment Score\")\n",
    "plt.ylabel(\"Mean Sentiment\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"../figures/weekly_mean_sentiment.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_by_entity = []\n",
    "for entity in tqdm(frequent_entities.index, desc=\"Extracting entities in int cols\"):\n",
    "    positive = df.loc[df[entity] == 1, \"Sentiment_positive\"].mean()\n",
    "    negative = df.loc[df[entity] == 1, \"Sentiment_negative\"].mean()\n",
    "    sentiments_by_entity.append(pd.Series([positive, negative], [\"positive\", \"negative\"], name=entity))\n",
    "sentiments_by_entity = pd.DataFrame(sentiments_by_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_by_entity.sort_values(\"positive\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_by_entity.sort_values(\"negative\", ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sentiments_by_entity[\"positive\"], sentiments_by_entity[\"negative\"])\n",
    "plt.xlabel(\"Mean Positive Sentiment Score\")\n",
    "plt.ylabel(\"Mean Negative Sentiment Score\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
